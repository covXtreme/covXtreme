%The details of the model can be found in Appendix \ref{App1}.   

\section{Appendix 1 : Model details} \label{App1}

%
This section contains details of the underlying statistical models used. 

%Stuff to do with both responses + a covariate
 Consider a sample $\{ \dot{y}_{1i},\dot{y}_{2i},\ldots \dot{y}_{Di}\}_{i=1}^{D}$ of $D$ joint values of peaks over threshold $\dot{Y}_1$ for a conditioning variate, and a number of associated conditioned variates $\dot{Y}_j$ (where $j \in \{1,\ldots,D\}$). Further, let $\{\boldsymbol{x}_i\}_{i=1}^{N}$ be the values of an associated covariate on some covariate domain $\mathcal{X}$.
 
 Each sample vector $\{ \dot{y}_{1i},\dot{y}_{2i},\ldots,\dot{y}_{Di}\}$ is allocated to one of $B$ covariate bins (indexed using $b$) by means of an allocation vector $A$.  All joint-observations with the same covariate bin $b$ are assumed to have common extreme value characteristics.  The dependence structure between the variates on a transformed standard scale is then estimated using a conditional extremes model.
 
 Non-stationarity with respect to covariate bins is captured via a GP shape and H\&T $\alpha$ parameter which can vary across bins. The extent of variation is controlled via a roughness-penalty in the marginal and conditional extreme value likelihood functions. Note that the spatial location of bins in multiple covariates (i.e. their relative location on a grid) does not factor into the roughness-penalty. For example, a case with $4$ directional bins and $3$ seasonal bins still boils down to a likelihood which is a product over the total $B = 4\times 3 = 12$ bins. 
%
%-------------------------------------------------------------------------------
\subsection{Marginal model}


For variate $d \in \{1,\ldots,D\}$ with response data $\{\dot{y}_{di}\}_{i=1}^{n}$ and \emph{marginal} non-exceedence probability $\tau$, the likelihood function for the marginal PPC model is as follows: 
%
\[
\mathcal{L}(\tau) = \prod_{b=1}^{B}  \prod_{i;A(i)=b} 
\left\{ \begin{array}{ll}
	 \frac{1}{\nu_{b}} \left(1 + \frac{\xi}{\nu_{b}} \left(\yd_{di}-\psi_{b}\right) \right)^{-1/\xi-1} & \mbox{if $\yd_{di}>\psi_{b}(\tau)$};\\
	\frac{1}{\Gamma(\omega)\left(\frac{\kappa}{\omega}\right)^{\omega}} (\dot{y}_{di} - l_{b})^{\omega - 1} \exp \left(\frac{-\omega(\dot{y}_{di}-l_{b})}{\kappa} \right)   & \mbox{if $\yd_{di}\leq\psi_{b}(\tau)$},
		\end{array} \right. 
\]
where the upper equation is simply the Generalised Pareto density; and the lower is the 3-parameter Gamma density corresponding to the models used above and below the threshold $\psi$. The first product is over covariate bins and the second is over observations within the given covariate bin. 



Note that the threshold $\psi_{b}$ in bin $b$ is directly controlled by the non-exceedance probability $\tau$ and hence appears as a function $\psi_{b}(\tau)$. With this threshold dictating the subset of response data included in likelihood, the other parameters are also dependent on the choice of $\tau$ (and are presented as such in Section \ref{Sec:Stg3}). Conditioned on the choice of $\tau$ however, the maximum likelihood estimates of the parameters are fixed. For ease of presentation we therefore do not represent the remaining parameters as functions of $\tau$ here.  

Recall from Section \ref{Sec:Stg3} that we have an additional parameter, the smoothness penalty $\lambda$, which controls the extent to which the GP shape $\nu_{b}$ can vary across bins. Its optimal value, $\hat{\lambda}$, is chosen to maximise predictive performance using a k-fold cross-validation procedure, resulting in the following roughness-penalised negative log-likelihood function: 

\pbe
\ell(\tau, \lambda) = -\log{\mathcal{L}(\tau)} + \lambda \left( \frac{1}{B} \sum_{b=1}^B \nu_{b}^2 - \left[ \frac{1}{B} \sum_{b=1}^B  \nu_{b} \right]^2 \right).
\pee
Parameters are estimated to minimise the negative log likelihood, with the GP and Gamma parameters carried forward to subsequent inference being those associated with the optimal $\hat{\lambda}$ resulting from cross-validation.  

Note that we have likelihood functions of this form for each variate $d$, meaning that each variate will come with a different set of fitted parameters. We do not include a subscript over $d$ here for simplicity of presentation.  

 







%transform from standard to uniform margins using CDF
%case 'Gumbel'
%P = exp(-exp(-X));
%case 'Laplace'
%P = (X>0)-.5*sign(X).*exp(-abs(X));
%
%% inverse
%transform from uniform to standard margins
%using inverse CDF
%case 'Gumbel'
%X = -log(-log(P));
%case 'Laplace'
%

\subsection{Transformation to Standard Scale} \label{App2}
The marginal models are used to transform from the original scale to the standard scale (Gumbel or Laplace) using the probability integral transform. 


\begin{enumerate}
	\item First we transform data above the threshold to uniform scale using the Generalised-Pareto CDF which, with parameters $\xi, \nu$ and $\psi$, has cumulative distribution function:
	\[F_{GP}(\dot{y};\xi,\nu,\psi) = (1+\frac{\xi}{\nu} \left(\dot{y}-\psi\right) )^{-1/\xi}\]
	for $\dot{y} \in (\psi, \dot{y}^+]$  where $\dot{y}^+$ is the upper end point of the distribution. 
	Below the threshold, a gamma CDF is used: 
	\[F_{\Gamma}(\dot{y};\omega,\kappa,l) = {\frac{1}{\Gamma(\omega)}}\gamma \left(\omega,\frac{\omega(\dot{y}-l)}{\kappa }\right) \]
	where $\gamma(\cdot,\cdot)$ is the lower incomplete gamma function. Note that this an orthogonal parameterisarion taken from \cite{Cox87}.
	% Pdf: $$ f(\omega, \kappa)=(\omega)^(-\omega)/Gamma(\omega) x^(\omega-1) exp(-\omega x/\kappa)$$

	That is, we transform original scale data $\dot{y}$ to uniform scale $u$ via the following:
	\begin{align*}
	u = 
	\left\{ \begin{array}{ll}
	F_{GP}(\dot{y}) & \mbox{if $\dot{y} > \psi$};\\
	F_{\Gamma}(\dot{y}) & \mbox{if $\dot{y} \leq \psi$}.
	\end{array} \right. 
	\end{align*}
	

	\item Given uniformly distributed $u$, to transform to standard \textbf{Laplace} margins we use the inverse of the standard Laplace CDF:
	\begin{align*}
	    y = F^{-1}_{L}(u)= \text{sign}(0.5-u)\times\log(2\times \min(1-u,u)); 
	\end{align*}
	where 
	\begin{align*}
		\boldsymbol{1}(z) = 
		\left\{ \begin{array}{ll}
			1 & \mbox{if $z > 0$};\\
			0 & \mbox{if $z \leq 0$}.
		\end{array} \right. 
	\end{align*}
	For reference, this is derived from the Laplace CDF: 
	\begin{align*}
			F_{L}(z) = \boldsymbol{1}(z)-0.5\times\text{sign}(z)\times\exp(-|z|).\\ 
	\end{align*}
	To transform the uniform data $u$ to standard \textbf{Gumbel} margins we use the inverse of the standard Gumbel CDF:
	\begin{align*}
%		F_{GP}(z) = \exp(-\exp(-z))\\
		y = F^{-1}_{GP}(u)= \log(\log(-u))
	\end{align*} 
	This is derived from the Gumbel CDF: 
	\begin{align*}
		F_{GP}(z) = \exp(-\exp(-z)).\\
	\end{align*}
\end{enumerate}

Non stationary is captured by transforming each bin in turn with its associated parameters.
%-------------------------------------------------------------------------------
\subsection{Conditional model}


%
The Gumbel-scale sample $\{y_{1i},y_{2i},.., y_{Di}\}$ above some threshold $\phi$ of the conditioning variate $Y_1$ is used to estimate a stationary conditional extremes model with parameters $\boldsymbol{\alpha}, \boldsymbol{\beta}, \boldsymbol{\mu}$ and $\boldsymbol{\sigma}$
%
\pbe
(Y_2, Y_3 \dots | Y_1 = y_{1i}) = \boldsymbol{\alpha}_{k} y_{1i} + y_{1i}^{\boldsymbol{\beta}} \boldsymbol{W}   \text{ for all } i \text{ such that } y_{1i}>\phi(\tilde{\tau}),
\pee
%
where $\boldsymbol{W} \sim N(\boldsymbol{\mu}, \boldsymbol{\sigma}^2)$ is assumed for model estimation only. Threshold $\phi(\tilde{\tau})$ is defined as the quantile of the standard Gumbel distribution with H\&T non-exceedance probability $\tilde{\tau}$. Parameters are estimated to minimise the negative log-likelihood
%
\pbe
\tilde{\ell}(\tilde{\tau}) =  \sum_{d=2}^{D}\sum^B_{b=1}\sum_{\stackrel{i;b=A(i)}{y_{di}>\phi(\tilde{\tau})}} \left[ \log(2 \pi \sigma_{d}^2)+\frac{1}{2 \sigma_{d}^2} \left(y_{di} - \alpha_{db} y_{1i} - \mu_{d} y_{1i} ^{\beta_{d}}\right)^2 \right]
\pee

\pbe
\tilde{\ell}(\tilde{\tau},\tilde{\lambda}) = \tilde{\ell}(\tilde{\tau}) + \tilde{\lambda}\sum_{d=2}^{D} \left( \frac{1}{B} \sum_{b=1}^B \alpha_{db}^2 - \left[ \frac{1}{B} \sum_{b=1}^B  \alpha_{db} \right]^2 \right)
\pee
%
for each value of $\tilde{\tau}$. This is assuming a Normal distribution in the likelihood which is almost certainly not appropriate, however the distribution of $\boldsymbol{W}_{\tilde{\tau}}$ is then estimated from the sample $\{r_{di}\}$ of residuals from the fit for $d \in \{2,\ldots,D\}$:
%
\pbe
r_{di} = \frac{1}{\sigma_{d}} \left(y_{di} - \alpha_{db} y_{1i} - \mu_{d} y_{1i}^{\beta_{d}} \right) \text{ for all } i \text{ such that } y_{1i}>\phi(\tilde{\tau}) .
\pee
Below the threshold, $\phi(\tilde{\tau})$, data are re-sampled using an empirical CDF based on the ranks of the data. 

%
%-------------------------------------------------------------------------------
